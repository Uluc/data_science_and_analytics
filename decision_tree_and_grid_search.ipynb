{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "lab5 Uluc.ipynb.txt",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uluc/data_science_and_analytics/blob/main/decision_tree_and_grid_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIxZK-Rqdnmf",
        "outputId": "3b74535b-0b2f-434c-c733-ce9969fc4e5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIZ3wwFYdnmk"
      },
      "source": [
        "## Implement the function to calculate gini index given the fraction of two classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13xC3z__dnml",
        "outputId": "78cf39d6-29da-4bc9-b8b1-df50a647049d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def gini(p1, p2):\n",
        "  return 1 - (p1*p1 + p2*p2)\n",
        "    \n",
        "   \n",
        "x = np.arange(0.0, 1.0, 0.01)\n",
        "plt.plot(x, gini(x, 1-x))\n",
        "plt.ylim([0, 1.1])\n",
        "plt.xlabel('p(i=1)')\n",
        "plt.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
        "plt.ylabel('Gini Impurity')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by8fCGnWdnmn"
      },
      "source": [
        "-----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VtNIacLdnmo"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4PpYBO2dnmp"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgp_wWXqdnmr"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaexHqHCdnmt",
        "outputId": "2cc34e13-4717-45af-94f0-23a1c21c1508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "print(np.unique(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(120, 4) (120,) (30, 4) (30,)\n",
            "[0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3noXhXXZdnmw",
        "outputId": "f127e593-f016-41e4-de55-a33cbb608bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(DecisionTreeClassifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
            "\n",
            "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
            " |  A decision tree classifier.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <tree>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
            " |      The function to measure the quality of a split. Supported criteria are\n",
            " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
            " |  \n",
            " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
            " |      The strategy used to choose the split at each node. Supported\n",
            " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
            " |      the best random split.\n",
            " |  \n",
            " |  max_depth : int, default=None\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int or float, default=2\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int or float, default=1\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, default=0.0\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |          - If int, then consider `max_features` features at each split.\n",
            " |          - If float, then `max_features` is a fraction and\n",
            " |            `int(max_features * n_features)` features are considered at each\n",
            " |            split.\n",
            " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |          - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  random_state : int or RandomState, default=None\n",
            " |      If int, random_state is the seed used by the random number generator;\n",
            " |      If RandomState instance, random_state is the random number generator;\n",
            " |      If None, the random number generator is the RandomState instance used\n",
            " |      by `np.random`.\n",
            " |  \n",
            " |  max_leaf_nodes : int, default=None\n",
            " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, default=0.0\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  min_impurity_split : float, default=1e-7\n",
            " |      Threshold for early stopping in tree growth. A node will split\n",
            " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
            " |  \n",
            " |      .. deprecated:: 0.19\n",
            " |         ``min_impurity_split`` has been deprecated in favor of\n",
            " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
            " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
            " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
            " |  \n",
            " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If None, all classes are supposed to have weight one. For\n",
            " |      multi-output problems, a list of dicts can be provided in the same\n",
            " |      order as the columns of y.\n",
            " |  \n",
            " |      Note that for multioutput (including multilabel) weights should be\n",
            " |      defined for each class of every column in its own dict. For example,\n",
            " |      for four-class multilabel classification weights should be\n",
            " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            " |  \n",
            " |      For multi-output, the weights of each column of y will be multiplied.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |  presort : deprecated, default='deprecated'\n",
            " |      This parameter is deprecated and will be removed in v0.24.\n",
            " |  \n",
            " |      .. deprecated:: 0.22\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, default=0.0\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
            " |      The classes labels (single output problem),\n",
            " |      or a list of arrays of class labels (multi-output problem).\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The feature importances. The higher, the more important the\n",
            " |      feature. The importance of a feature is computed as the (normalized)\n",
            " |      total reduction of the criterion brought by that feature.  It is also\n",
            " |      known as the Gini importance [4]_.\n",
            " |  \n",
            " |  max_features_ : int\n",
            " |      The inferred value of max_features.\n",
            " |  \n",
            " |  n_classes_ : int or list of int\n",
            " |      The number of classes (for single output problems),\n",
            " |      or a list containing the number of classes for each\n",
            " |      output (for multi-output problems).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  tree_ : Tree\n",
            " |      The underlying Tree object. Please refer to\n",
            " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
            " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
            " |      for basic usage of these attributes.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  DecisionTreeRegressor : A decision tree regressor.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data and\n",
            " |  ``max_features=n_features``, if the improvement of the criterion is\n",
            " |  identical for several splits enumerated during the search of the best\n",
            " |  split. To obtain a deterministic behaviour during fitting,\n",
            " |  ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
            " |  \n",
            " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
            " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
            " |  \n",
            " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
            " |         Learning\", Springer, 2009.\n",
            " |  \n",
            " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
            " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.datasets import load_iris\n",
            " |  >>> from sklearn.model_selection import cross_val_score\n",
            " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
            " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
            " |  >>> iris = load_iris()\n",
            " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
            " |  ...                             # doctest: +SKIP\n",
            " |  ...\n",
            " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
            " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DecisionTreeClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseDecisionTree\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
            " |      Build a decision tree classifier from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      X_idx_sorted : array-like of shape (n_samples, n_features),                 default=None\n",
            " |          The indexes of the sorted training input samples. If many tree\n",
            " |          are grown on the same dataset, this allows the ordering to be\n",
            " |          cached between trees. If None, the data will be sorted here.\n",
            " |          Don't use this parameter unless you know what to do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : DecisionTreeClassifier\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict class log-probabilities of the input samples X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class log-probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X, check_input=True)\n",
            " |      Predict class probabilities of the input samples X.\n",
            " |      \n",
            " |      The predicted class probability is the fraction of samples of the same\n",
            " |      class in a leaf.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  apply(self, X, check_input=True)\n",
            " |      Return the index of the leaf that each sample is predicted as.\n",
            " |      \n",
            " |      .. versionadded:: 0.17\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array-like of shape (n_samples,)\n",
            " |          For each datapoint x in X, return the index of the leaf x\n",
            " |          ends up in. Leaves are numbered within\n",
            " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
            " |          numbering.\n",
            " |  \n",
            " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
            " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
            " |      \n",
            " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
            " |      process.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ccp_path : Bunch\n",
            " |          Dictionary-like object, with attributes:\n",
            " |      \n",
            " |          ccp_alphas : ndarray\n",
            " |              Effective alphas of subtree during pruning.\n",
            " |      \n",
            " |          impurities : ndarray\n",
            " |              Sum of the impurities of the subtree leaves for the\n",
            " |              corresponding alpha value in ``ccp_alphas``.\n",
            " |  \n",
            " |  decision_path(self, X, check_input=True)\n",
            " |      Return the decision path in the tree.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
            " |          Return a node indicator CSR matrix where non zero elements\n",
            " |          indicates that the samples goes through the nodes.\n",
            " |  \n",
            " |  get_depth(self)\n",
            " |      Return the depth of the decision tree.\n",
            " |      \n",
            " |      The depth of a tree is the maximum distance between the root\n",
            " |      and any leaf.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.max_depth : int\n",
            " |          The maximum depth of the tree.\n",
            " |  \n",
            " |  get_n_leaves(self)\n",
            " |      Return the number of leaves of the decision tree.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.n_leaves : int\n",
            " |          Number of leaves.\n",
            " |  \n",
            " |  predict(self, X, check_input=True)\n",
            " |      Predict class or regression value for X.\n",
            " |      \n",
            " |      For a classification model, the predicted class for each sample in X is\n",
            " |      returned. For a regression model, the predicted value based on X is\n",
            " |      returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted classes, or the predict values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Return the feature importances.\n",
            " |      \n",
            " |      The importance of a feature is computed as the (normalized) total\n",
            " |      reduction of the criterion brought by that feature.\n",
            " |      It is also known as the Gini importance.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : ndarray of shape (n_features,)\n",
            " |          Normalized total reduction of criteria by feature\n",
            " |          (Gini importance).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ToJtw3Jydnmy"
      },
      "source": [
        "Task 1\n",
        "---\n",
        "Build a decision tree model using the training data (X_train, y_train). Use the model to make prediction on the testing data (X_test). Calculate the *accuracy* of the prediction comparing to the treu class label (y_test). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A773FFK8dnmz",
        "outputId": "57af7f1a-2d93-4c90-8f1a-5a81bc814f1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train, y_train)\n",
        "p = tree.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,p))\n",
        "\n",
        "print(np.sum(y_test == p)/len(p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9666666666666667\n",
            "0.9666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAjN6TKodnm1"
      },
      "source": [
        "Task 2\n",
        "---\n",
        "Using GridSearchCV, do a search to find the values for 'min_samples_leaf' (1, 5, 10) and 'max_depth' (2, 3, 4) that give the best prediction accuracy. Print the grid scores and the best values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgiIqamidnm1",
        "outputId": "b5709a22-2f3b-41fb-b66d-9f31b38b4fbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "help(GridSearchCV)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
            "\n",
            "class GridSearchCV(BaseSearchCV)\n",
            " |  Exhaustive search over specified parameter values for an estimator.\n",
            " |  \n",
            " |  Important members are fit, predict.\n",
            " |  \n",
            " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
            " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
            " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
            " |  estimator used.\n",
            " |  \n",
            " |  The parameters of the estimator used to apply these methods are optimized\n",
            " |  by cross-validated grid-search over a parameter grid.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <grid_search>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  estimator : estimator object.\n",
            " |      This is assumed to implement the scikit-learn estimator interface.\n",
            " |      Either estimator needs to provide a ``score`` function,\n",
            " |      or ``scoring`` must be passed.\n",
            " |  \n",
            " |  param_grid : dict or list of dictionaries\n",
            " |      Dictionary with parameters names (string) as keys and lists of\n",
            " |      parameter settings to try as values, or a list of such\n",
            " |      dictionaries, in which case the grids spanned by each dictionary\n",
            " |      in the list are explored. This enables searching over any sequence\n",
            " |      of parameter settings.\n",
            " |  \n",
            " |  scoring : string, callable, list/tuple, dict or None, default: None\n",
            " |      A single string (see :ref:`scoring_parameter`) or a callable\n",
            " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
            " |  \n",
            " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
            " |      or a dict with names as keys and callables as values.\n",
            " |  \n",
            " |      NOTE that when using custom scorers, each scorer should return a single\n",
            " |      value. Metric functions returning a list/array of values can be wrapped\n",
            " |      into multiple scorers that return one value each.\n",
            " |  \n",
            " |      See :ref:`multimetric_grid_search` for an example.\n",
            " |  \n",
            " |      If None, the estimator's score method is used.\n",
            " |  \n",
            " |  n_jobs : int or None, optional (default=None)\n",
            " |      Number of jobs to run in parallel.\n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |  \n",
            " |  pre_dispatch : int, or string, optional\n",
            " |      Controls the number of jobs that get dispatched during parallel\n",
            " |      execution. Reducing this number can be useful to avoid an\n",
            " |      explosion of memory consumption when more jobs get dispatched\n",
            " |      than CPUs can process. This parameter can be:\n",
            " |  \n",
            " |          - None, in which case all the jobs are immediately\n",
            " |            created and spawned. Use this for lightweight and\n",
            " |            fast-running jobs, to avoid delays due to on-demand\n",
            " |            spawning of the jobs\n",
            " |  \n",
            " |          - An int, giving the exact number of total jobs that are\n",
            " |            spawned\n",
            " |  \n",
            " |          - A string, giving an expression as a function of n_jobs,\n",
            " |            as in '2*n_jobs'\n",
            " |  \n",
            " |  iid : boolean, default=False\n",
            " |      If True, return the average score across folds, weighted by the number\n",
            " |      of samples in each test set. In this case, the data is assumed to be\n",
            " |      identically distributed across the folds, and the loss minimized is\n",
            " |      the total loss per sample, and not the mean loss across the folds.\n",
            " |  \n",
            " |      .. deprecated:: 0.22\n",
            " |          Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n",
            " |  \n",
            " |  cv : int, cross-validation generator or an iterable, optional\n",
            " |      Determines the cross-validation splitting strategy.\n",
            " |      Possible inputs for cv are:\n",
            " |  \n",
            " |      - None, to use the default 5-fold cross validation,\n",
            " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
            " |      - :term:`CV splitter`,\n",
            " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
            " |  \n",
            " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
            " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
            " |      other cases, :class:`KFold` is used.\n",
            " |  \n",
            " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
            " |      cross-validation strategies that can be used here.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
            " |  \n",
            " |  refit : boolean, string, or callable, default=True\n",
            " |      Refit an estimator using the best found parameters on the whole\n",
            " |      dataset.\n",
            " |  \n",
            " |      For multiple metric evaluation, this needs to be a string denoting the\n",
            " |      scorer that would be used to find the best parameters for refitting\n",
            " |      the estimator at the end.\n",
            " |  \n",
            " |      Where there are considerations other than maximum score in\n",
            " |      choosing a best estimator, ``refit`` can be set to a function which\n",
            " |      returns the selected ``best_index_`` given ``cv_results_``. In that\n",
            " |      case, the ``best_estimator_`` and ``best_parameters_`` will be set\n",
            " |      according to the returned ``best_index_`` while the ``best_score_``\n",
            " |      attribute will not be available.\n",
            " |  \n",
            " |      The refitted estimator is made available at the ``best_estimator_``\n",
            " |      attribute and permits using ``predict`` directly on this\n",
            " |      ``GridSearchCV`` instance.\n",
            " |  \n",
            " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
            " |      ``best_score_`` and ``best_params_`` will only be available if\n",
            " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
            " |      scorer.\n",
            " |  \n",
            " |      See ``scoring`` parameter to know more about multiple metric\n",
            " |      evaluation.\n",
            " |  \n",
            " |      .. versionchanged:: 0.20\n",
            " |          Support for callable added.\n",
            " |  \n",
            " |  verbose : integer\n",
            " |      Controls the verbosity: the higher, the more messages.\n",
            " |  \n",
            " |  error_score : 'raise' or numeric\n",
            " |      Value to assign to the score if an error occurs in estimator fitting.\n",
            " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
            " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
            " |      step, which will always raise the error. Default is ``np.nan``.\n",
            " |  \n",
            " |  return_train_score : boolean, default=False\n",
            " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
            " |      scores.\n",
            " |      Computing training scores is used to get insights on how different\n",
            " |      parameter settings impact the overfitting/underfitting trade-off.\n",
            " |      However computing the scores on the training set can be computationally\n",
            " |      expensive and is not strictly required to select the parameters that\n",
            " |      yield the best generalization performance.\n",
            " |  \n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn import svm, datasets\n",
            " |  >>> from sklearn.model_selection import GridSearchCV\n",
            " |  >>> iris = datasets.load_iris()\n",
            " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
            " |  >>> svc = svm.SVC()\n",
            " |  >>> clf = GridSearchCV(svc, parameters)\n",
            " |  >>> clf.fit(iris.data, iris.target)\n",
            " |  GridSearchCV(estimator=SVC(),\n",
            " |               param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
            " |  >>> sorted(clf.cv_results_.keys())\n",
            " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
            " |   'param_C', 'param_kernel', 'params',...\n",
            " |   'rank_test_score', 'split0_test_score',...\n",
            " |   'split2_test_score', ...\n",
            " |   'std_fit_time', 'std_score_time', 'std_test_score']\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  cv_results_ : dict of numpy (masked) ndarrays\n",
            " |      A dict with keys as column headers and values as columns, that can be\n",
            " |      imported into a pandas ``DataFrame``.\n",
            " |  \n",
            " |      For instance the below given table\n",
            " |  \n",
            " |      +------------+-----------+------------+-----------------+---+---------+\n",
            " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
            " |      +============+===========+============+=================+===+=========+\n",
            " |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
            " |      +------------+-----------+------------+-----------------+---+---------+\n",
            " |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
            " |      +------------+-----------+------------+-----------------+---+---------+\n",
            " |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
            " |      +------------+-----------+------------+-----------------+---+---------+\n",
            " |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
            " |      +------------+-----------+------------+-----------------+---+---------+\n",
            " |  \n",
            " |      will be represented by a ``cv_results_`` dict of::\n",
            " |  \n",
            " |          {\n",
            " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
            " |                                       mask = [False False False False]...)\n",
            " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
            " |                                      mask = [ True  True False False]...),\n",
            " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
            " |                                       mask = [False False  True  True]...),\n",
            " |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
            " |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
            " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
            " |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
            " |          'rank_test_score'    : [2, 4, 3, 1],\n",
            " |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
            " |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
            " |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
            " |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
            " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
            " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
            " |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
            " |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
            " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
            " |          }\n",
            " |  \n",
            " |      NOTE\n",
            " |  \n",
            " |      The key ``'params'`` is used to store a list of parameter\n",
            " |      settings dicts for all the parameter candidates.\n",
            " |  \n",
            " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
            " |      ``std_score_time`` are all in seconds.\n",
            " |  \n",
            " |      For multi-metric evaluation, the scores for all the scorers are\n",
            " |      available in the ``cv_results_`` dict at the keys ending with that\n",
            " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
            " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
            " |  \n",
            " |  best_estimator_ : estimator\n",
            " |      Estimator that was chosen by the search, i.e. estimator\n",
            " |      which gave highest score (or smallest loss if specified)\n",
            " |      on the left out data. Not available if ``refit=False``.\n",
            " |  \n",
            " |      See ``refit`` parameter for more information on allowed values.\n",
            " |  \n",
            " |  best_score_ : float\n",
            " |      Mean cross-validated score of the best_estimator\n",
            " |  \n",
            " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
            " |      specified.\n",
            " |  \n",
            " |      This attribute is not available if ``refit`` is a function.\n",
            " |  \n",
            " |  best_params_ : dict\n",
            " |      Parameter setting that gave the best results on the hold out data.\n",
            " |  \n",
            " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
            " |      specified.\n",
            " |  \n",
            " |  best_index_ : int\n",
            " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
            " |      candidate parameter setting.\n",
            " |  \n",
            " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
            " |      the parameter setting for the best model, that gives the highest\n",
            " |      mean score (``search.best_score_``).\n",
            " |  \n",
            " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
            " |      specified.\n",
            " |  \n",
            " |  scorer_ : function or a dict\n",
            " |      Scorer function used on the held out data to choose the best\n",
            " |      parameters for the model.\n",
            " |  \n",
            " |      For multi-metric evaluation, this attribute holds the validated\n",
            " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
            " |  \n",
            " |  n_splits_ : int\n",
            " |      The number of cross-validation splits (folds/iterations).\n",
            " |  \n",
            " |  refit_time_ : float\n",
            " |      Seconds used for refitting the best model on the whole dataset.\n",
            " |  \n",
            " |      This is present only if ``refit`` is not False.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The parameters selected are those that maximize the score of the left out\n",
            " |  data, unless an explicit score is passed in which case it is used instead.\n",
            " |  \n",
            " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
            " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
            " |  reasons if individual jobs take very little time, but may raise errors if\n",
            " |  the dataset is large and not enough memory is available.  A workaround in\n",
            " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
            " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
            " |  n_jobs`.\n",
            " |  \n",
            " |  See Also\n",
            " |  ---------\n",
            " |  :class:`ParameterGrid`:\n",
            " |      generates all the combinations of a hyperparameter grid.\n",
            " |  \n",
            " |  :func:`sklearn.model_selection.train_test_split`:\n",
            " |      utility function to split the data into a development set usable\n",
            " |      for fitting a GridSearchCV instance and an evaluation set for\n",
            " |      its final evaluation.\n",
            " |  \n",
            " |  :func:`sklearn.metrics.make_scorer`:\n",
            " |      Make a scorer from a performance metric or loss function.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      GridSearchCV\n",
            " |      BaseSearchCV\n",
            " |      sklearn.base.MetaEstimatorMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, estimator, param_grid, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseSearchCV:\n",
            " |  \n",
            " |  decision_function(self, X)\n",
            " |      Call decision_function on the estimator with the best found parameters.\n",
            " |      \n",
            " |      Only available if ``refit=True`` and the underlying estimator supports\n",
            " |      ``decision_function``.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : indexable, length n_samples\n",
            " |          Must fulfill the input assumptions of the\n",
            " |          underlying estimator.\n",
            " |  \n",
            " |  fit(self, X, y=None, groups=None, **fit_params)\n",
            " |      Run fit with all sets of parameters.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Training vector, where n_samples is the number of samples and\n",
            " |          n_features is the number of features.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples, n_output) or (n_samples,), optional\n",
            " |          Target relative to X for classification or regression;\n",
            " |          None for unsupervised learning.\n",
            " |      \n",
            " |      groups : array-like, with shape (n_samples,), optional\n",
            " |          Group labels for the samples used while splitting the dataset into\n",
            " |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
            " |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
            " |      \n",
            " |      **fit_params : dict of string -> object\n",
            " |          Parameters passed to the ``fit`` method of the estimator\n",
            " |  \n",
            " |  inverse_transform(self, Xt)\n",
            " |      Call inverse_transform on the estimator with the best found params.\n",
            " |      \n",
            " |      Only available if the underlying estimator implements\n",
            " |      ``inverse_transform`` and ``refit=True``.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      Xt : indexable, length n_samples\n",
            " |          Must fulfill the input assumptions of the\n",
            " |          underlying estimator.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Call predict on the estimator with the best found parameters.\n",
            " |      \n",
            " |      Only available if ``refit=True`` and the underlying estimator supports\n",
            " |      ``predict``.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : indexable, length n_samples\n",
            " |          Must fulfill the input assumptions of the\n",
            " |          underlying estimator.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Call predict_log_proba on the estimator with the best found parameters.\n",
            " |      \n",
            " |      Only available if ``refit=True`` and the underlying estimator supports\n",
            " |      ``predict_log_proba``.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : indexable, length n_samples\n",
            " |          Must fulfill the input assumptions of the\n",
            " |          underlying estimator.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Call predict_proba on the estimator with the best found parameters.\n",
            " |      \n",
            " |      Only available if ``refit=True`` and the underlying estimator supports\n",
            " |      ``predict_proba``.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : indexable, length n_samples\n",
            " |          Must fulfill the input assumptions of the\n",
            " |          underlying estimator.\n",
            " |  \n",
            " |  score(self, X, y=None)\n",
            " |      Returns the score on the given data, if the estimator has been refit.\n",
            " |      \n",
            " |      This uses the score defined by ``scoring`` where provided, and the\n",
            " |      ``best_estimator_.score`` method otherwise.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Input data, where n_samples is the number of samples and\n",
            " |          n_features is the number of features.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples, n_output) or (n_samples,), optional\n",
            " |          Target relative to X for classification or regression;\n",
            " |          None for unsupervised learning.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |  \n",
            " |  transform(self, X)\n",
            " |      Call transform on the estimator with the best found parameters.\n",
            " |      \n",
            " |      Only available if the underlying estimator supports ``transform`` and\n",
            " |      ``refit=True``.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : indexable, length n_samples\n",
            " |          Must fulfill the input assumptions of the\n",
            " |          underlying estimator.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseSearchCV:\n",
            " |  \n",
            " |  classes_\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S74BUGjxdnm3",
        "outputId": "889dd975-55da-4c0c-d109-d24ea075629f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "tree_clf = DecisionTreeClassifier()\n",
        "tree_search_params = {'min_samples_leaf':[1,5,10], 'max_depth':[2,3,4]}\n",
        "tree_search = GridSearchCV(tree_clf, tree_search_params, cv=5, verbose=0)\n",
        "tree_search.fit(X,y)\n",
        "\n",
        "print('Best score: ',tree_search.best_score_)\n",
        "print('Best parameters: ',tree_search.best_params_)\n",
        "\n",
        "import pandas as pd\n",
        "score = pd.DataFrame(tree_search.cv_results_)\n",
        "score[score.columns[6:13]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score:  0.9733333333333334\n",
            "Best parameters:  {'max_depth': 3, 'min_samples_leaf': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'max_depth': 2, 'min_samples_leaf': 1}</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'max_depth': 2, 'min_samples_leaf': 5}</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'max_depth': 2, 'min_samples_leaf': 10}</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'max_depth': 3, 'min_samples_leaf': 1}</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.973333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'max_depth': 3, 'min_samples_leaf': 5}</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>{'max_depth': 3, 'min_samples_leaf': 10}</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>{'max_depth': 4, 'min_samples_leaf': 1}</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.953333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>{'max_depth': 4, 'min_samples_leaf': 5}</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>{'max_depth': 4, 'min_samples_leaf': 10}</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     params  ...  mean_test_score\n",
              "0   {'max_depth': 2, 'min_samples_leaf': 1}  ...         0.933333\n",
              "1   {'max_depth': 2, 'min_samples_leaf': 5}  ...         0.933333\n",
              "2  {'max_depth': 2, 'min_samples_leaf': 10}  ...         0.933333\n",
              "3   {'max_depth': 3, 'min_samples_leaf': 1}  ...         0.973333\n",
              "4   {'max_depth': 3, 'min_samples_leaf': 5}  ...         0.940000\n",
              "5  {'max_depth': 3, 'min_samples_leaf': 10}  ...         0.933333\n",
              "6   {'max_depth': 4, 'min_samples_leaf': 1}  ...         0.953333\n",
              "7   {'max_depth': 4, 'min_samples_leaf': 5}  ...         0.940000\n",
              "8  {'max_depth': 4, 'min_samples_leaf': 10}  ...         0.933333\n",
              "\n",
              "[9 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHZmoqfDdnm5"
      },
      "source": [
        "-----------------------------------------------------------------------"
      ]
    }
  ]
}